---
layout: post
title:  "OpenAI Emails"
date:   2024-11-21 0:00:00 -0400
categories:
  - ai-safety
---

The emails can be found [here](https://www.lesswrong.com/posts/5jjk4CDnj9tA7ugxr/openai-email-archives-from-musk-v-altman)

# Notes and takeaways
- Altman seems reasonably aligned, but this failure mode of "act" seems really popular; I don't think OpenAI has been net positive. 
- "It is super important to get the opening summary section right. This will be what everyone reads and what the press mostly quotes. The whole point of this release is to attract top talent. Not sure Greg totally gets that." - Ok, interesting. 
- "We are outmanned and outgunned by a ridiculous margin by organizations you know well, but we have right on our side and that counts for a lot. I like the odds." - wow, mission can be really powerful here. 
- Everything is about talent. It's remarkable the winner-take-all effects here. And it's pretty interesting to see how focused EM is of optimizing for them...
- "History unequivocally illustrates that a powerful technology is a double-edged sword. It would be foolish to assume that AI, arguably the most powerful of all technologies, only has a single edge." - I mean... what a nice quote.
- "This process has been the highest stakes conversation that Greg and I have ever participated in, and if the project succeeds, it'll turn out to have been the highest stakes conversation the world has seen. It's also been a deeply personal conversation for all of us." - wow, this is wild. 
- Ilya to Sam: "We don't understand why the CEO title is so important to you. Your stated reasons have changed, and it's hard to really understand what's driving it." - Sam seems like an exceptional political navigator.
- "I have considered the ICO approach and will not support it. In my opinion, that would simply result in a massive loss of credibility for OpenAI and everyone associated with the ICO. If something seems too good to be true, it is. This was, in my opinion, an unwise diversion." - lol

# Takeaways
1. I can't really tell how much missing context there is here, but it seems reasonably clear that from the very start there's been a large gap in trust - perhaps this was never repaired (how could you).
2. No one really was deluding themselves about the stakes of the conversation. Everyone was pretty much onboard with where they were. 
3. Mission committment clearly was not enough. Just saying that you are going to try to broadly distribute benefits doesn't actually mean that you will - and in practice, it's not clear to me how this shifted. 

# Questions I have
1. Why will the same thing not happen to Anthropic? The early story seems quite similar to what we see with OpenAI (without the non-profit structure even), so what really is the difference. 
2. Calling out the race dynamics seems like it perhaps contributes to the race dynamics. How much of this race did we build for ourselves? If Bostrom never wrote his book, would we even be here?
3. Where has Elon ended up? His early statements seem reasonably aligned with what one would want, but it's not clear that this is even enough. 
4. Is it just neuroticism to worry about the impact of your work? I think probably not - but coming up with a specific way to justify what's worth worrying about vs. not seems pretty much impossible.  
