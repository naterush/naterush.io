---
layout: post
title:  "My first half iron"
date:   2024-7-9 0:00:00 -0400
categories:
  - ai-safety
---

Why I'm suspicious of interpretability:
1. There's the abstract argument that it will learn things we can't understand.
2. There's a more specific argument: every interpretability method we have so far is lossy.
3. At best, we're going to get to a lossy representation. 

This isn't true if the model is built to be interpetable, like a decision tree. 

But a decision tree is different because... it's architecture is specifically designed for human understanding. This is not the case with architectures we know work for LLMs, like transformers...

If interpretability of the model that work go out the door, then what? 